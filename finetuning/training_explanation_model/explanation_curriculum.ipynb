{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17258,"status":"ok","timestamp":1749455835258,"user":{"displayName":"최승규","userId":"10957631576951172627"},"user_tz":-540},"id":"atcO-LD6Swog","outputId":"05b7ce65-c021-440b-c051-39aae98cc7c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2450,"status":"ok","timestamp":1749455837722,"user":{"displayName":"최승규","userId":"10957631576951172627"},"user_tz":-540},"id":"Eu5R0PrEVSb8","outputId":"bc86416f-fe22-471c-c91c-4af608330c3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'public_cs224n_gpt'...\n","remote: Enumerating objects: 69, done.\u001b[K\n","remote: Counting objects: 100% (37/37), done.\u001b[K\n","remote: Compressing objects: 100% (24/24), done.\u001b[K\n","remote: Total 69 (delta 21), reused 13 (delta 13), pack-reused 32 (from 1)\u001b[K\n","Receiving objects: 100% (69/69), 30.87 MiB | 27.07 MiB/s, done.\n","Resolving deltas: 100% (22/22), done.\n","/content/public_cs224n_gpt\n"]}],"source":["!git clone https://github.com/ItWasAllYellow/public_cs224n_gpt.git\n","\n","%cd /content/public_cs224n_gpt"]},{"cell_type":"code","source":["# eval_expl_all_curr.py\n","\"\"\"\n","Evaluate fine-tuned GPT-2 explainer (all options) on test split:\n","  1) Gold-target perplexity\n","  2) BLEU score of generated explanation vs. gold\n","  3) Save CSV with input, gold, pred to:\n","       /content/drive/MyDrive/CSEG321/explanation_all_options_test_{version}.csv\n","\"\"\"\n","\n","import argparse, os, math\n","import numpy as np, pandas as pd, torch\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","from transformers import GPT2Tokenizer\n","from models.gpt2 import GPT2Model\n","import torch.nn.functional as F\n","from torch import nn\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","\n","# ────────────────────────────────────────────────────────────────────────────── #\n","class ExplDataset(Dataset):\n","    def __init__(self, df, tok, max_len=512):\n","        self.df, self.tok, self.max_len = df.reset_index(drop=True), tok, max_len\n","    def __len__(self): return len(self.df)\n","    def __getitem__(self, idx):\n","        r = self.df.iloc[idx]\n","        return r[\"input\"], r[\"target\"]\n","    def collate_fn(self, batch):\n","        ins, tgs = zip(*batch)\n","        enc_in = self.tok(list(ins),\n","                          padding=\"max_length\", truncation=True,\n","                          max_length=self.max_len, return_tensors=\"pt\")\n","        enc_tg = self.tok(list(tgs),\n","                          padding=\"max_length\", truncation=True,\n","                          max_length=self.max_len, return_tensors=\"pt\")\n","        labels = enc_tg.input_ids.clone()\n","        labels[enc_tg.attention_mask == 0] = -100\n","        return {\n","            \"input_ids\": enc_in.input_ids,\n","            \"attention_mask\": enc_in.attention_mask,\n","            \"labels\": labels,\n","            \"raw_input\": ins,\n","            \"raw_target\": tgs\n","        }\n","\n","class GPT2Explainer(nn.Module):\n","    def __init__(self, args):\n","        super().__init__()\n","        self.backbone = GPT2Model.from_pretrained(\n","            model=args.model_size, d=args.d, l=args.l, num_heads=args.num_heads\n","        )\n","        vocab = GPT2Tokenizer.from_pretrained(args.model_size).vocab_size\n","        self.lm_head = nn.Linear(args.d, vocab, bias=False)\n","    def forward(self, ids, mask, labels=None):\n","        h = self.backbone(ids, attention_mask=mask)[\"last_hidden_state\"]\n","        logits = self.lm_head(h)  # (B, L, V)\n","        if labels is None:\n","            return logits\n","        loss = F.cross_entropy(\n","            logits.view(-1, logits.size(-1)),\n","            labels.view(-1),\n","            ignore_index=-100,\n","            reduction=\"sum\"\n","        )\n","        # return sum NLL and token count for PPL\n","        tok_cnt = (labels != -100).sum()\n","        return loss, tok_cnt, logits\n","\n","    def generate(self, ids, mask, max_gen_len=100):\n","        \"\"\"Greedy decode until eos_token_id or max_gen_len.\"\"\"\n","        bs, seq_len = ids.size()\n","        generated = ids.clone()\n","        attention = mask.clone()\n","        eos = self.backbone.config.eos_token_id if hasattr(self.backbone.config, \"eos_token_id\") else self.lm_head.weight.device\n","        for _ in range(max_gen_len):\n","            h = self.backbone(generated, attention_mask=attention)[\"last_hidden_state\"]\n","            logits = self.lm_head(h)  # (B, T, V)\n","            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)  # (B,1)\n","            generated = torch.cat([generated, next_token], dim=1)\n","            attention = torch.cat([attention, torch.ones(bs,1,device=attention.device)], dim=1)\n","            if (next_token == self.backbone.config.eos_token_id).all():\n","                break\n","        return generated  # full sequence including prompt\n","\n","def evaluate(args):\n","    # init\n","    device = torch.device(\"cuda\" if (args.use_gpu and torch.cuda.is_available()) else \"cpu\")\n","    tok = GPT2Tokenizer.from_pretrained(args.model_size)\n","    tok.pad_token = tok.eos_token\n","    # load model\n","    ckpt = torch.load(args.model_path, map_location=\"cpu\", weights_only=False)\n","    model = GPT2Explainer(args).to(device)\n","    model.load_state_dict(ckpt[\"model\"])\n","    model.eval()\n","\n","    # data\n","    df = pd.read_csv(args.data_path, encoding=\"utf-8-sig\")\n","    test_df = df[df.split == \"test\"]\n","    ds = ExplDataset(test_df, tok, max_len=args.max_length)\n","    loader = DataLoader(ds, batch_size=args.batch_size, shuffle=False, collate_fn=ds.collate_fn)\n","\n","    # metrics accumulators\n","    total_loss = 0.0; total_tokens = 0\n","    bleu_scores = []\n","    hyp_list = []; ref_list = []; inp_list = []\n","    version = args.version\n","    output_rows = []\n","\n","    smooth = SmoothingFunction().method1\n","\n","    # evaluate\n","    with torch.no_grad():\n","        for batch in tqdm(loader, desc=\"Evaluating\"):\n","            ids = batch[\"input_ids\"].to(device)\n","            mask= batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","            # PPL on gold\n","            loss_sum, tok_cnt, _ = model(ids, mask, labels)\n","            total_loss += loss_sum.item()\n","            total_tokens += tok_cnt.item()\n","            # generate\n","            gen_ids = model.generate(ids, mask, max_gen_len=args.gen_max_length)\n","            # slice off prompt length\n","            bs, prompt_len = ids.size()\n","            for i in range(bs):\n","                inp = batch[\"raw_input\"][i]\n","                ref = batch[\"raw_target\"][i]\n","                full = gen_ids[i].cpu().tolist()\n","                # skip prompt tokens\n","                pred_tokens = full[prompt_len:]\n","                # truncate at eos\n","                if tok.eos_token_id in pred_tokens:\n","                    pred_tokens = pred_tokens[: pred_tokens.index(tok.eos_token_id)]\n","                pred = tok.decode(pred_tokens, skip_special_tokens=True).strip()\n","                # BLEU\n","                bleu = sentence_bleu([ref.split()], pred.split(), smoothing_function=smooth)\n","                bleu_scores.append(bleu)\n","                # save row\n","                output_rows.append({\n","                    \"input\": inp,\n","                    \"target\": ref,\n","                    \"prediction\": pred\n","                })\n","\n","    # final metrics\n","    ppl = math.exp(total_loss / total_tokens)\n","    avg_bleu = np.mean(bleu_scores)\n","\n","    print(f\"Test PPL:  {ppl:.3f}\")\n","    print(f\"Avg BLEU:  {avg_bleu:.3f}\")\n","\n","    # save CSV\n","    out_df = pd.DataFrame(output_rows)\n","    os.makedirs(os.path.dirname(args.save_path), exist_ok=True)\n","    out_df.to_csv(args.save_path.format(version=version), index=False, encoding=\"utf-8-sig\")\n","    print(\"Saved outputs to\", args.save_path.format(version=version))\n","\n","if __name__ == \"__main__\":\n","    p = argparse.ArgumentParser()\n","    p.add_argument(\"--data_path\",      type=str, default=\"/content/drive/MyDrive/CSEG321/dataset/explanation_all_options.csv\")\n","    p.add_argument(\"--model_path\",     type=str, required=True)\n","    p.add_argument(\"--save_path\",      type=str,\n","                   default=\"/content/drive/MyDrive/CSEG321/explanation_all_options_test_{version}.csv\")\n","    p.add_argument(\"--model_size\",     type=str, default=\"gpt2\",\n","                   choices=[\"gpt2\",\"gpt2-medium\",\"gpt2-large\"])\n","    p.add_argument(\"--batch_size\",     type=int, default=4)\n","    p.add_argument(\"--max_length\",     type=int, default=512)\n","    p.add_argument(\"--gen_max_length\", type=int, default=100)\n","    p.add_argument(\"--version\",        type=str, required=True,\n","                   help=\"Identifier for this run, e.g., 'cot'\")\n","    p.add_argument(\"--use_gpu\",        action=\"store_true\")\n","    args = p.parse_args([])\n","    evaluate(args)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"id":"INdU9cC0sf36","executionInfo":{"status":"error","timestamp":1749456066239,"user_tz":-540,"elapsed":64,"user":{"displayName":"최승규","userId":"10957631576951172627"}},"outputId":"1cbb0cee-b055-496c-d49e-6da4794148cb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["usage: colab_kernel_launcher.py [-h] [--data_path DATA_PATH] --model_path\n","                                MODEL_PATH [--save_path SAVE_PATH]\n","                                [--model_size {gpt2,gpt2-medium,gpt2-large}]\n","                                [--batch_size BATCH_SIZE]\n","                                [--max_length MAX_LENGTH]\n","                                [--gen_max_length GEN_MAX_LENGTH] --version\n","                                VERSION [--use_gpu]\n","colab_kernel_launcher.py: error: the following arguments are required: --model_path, --version\n"]},{"output_type":"error","ename":"SystemExit","evalue":"2","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNRIH8fF3PkURTnAcBpIfYJ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}